{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hirwabrian/Group19-Machine_Learning_Techniques_I/blob/main/LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89adffdf",
        "outputId": "7f6b7d87-abb2-4973-bc70-d02363685c19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cs2fVXGa03qF",
        "outputId": "5b6fe764-ab21-4a03-f2d8-ce0edef554ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaning /content/drive/MyDrive/AG News Classification Dataset/train.csv...\n",
            "Cleaning /content/drive/MyDrive/AG News Classification Dataset/test.csv...\n",
            "\n",
            "Sample Output:\n",
            "                                                text  label class_name\n",
            "0  wall st bears claw back into the black reuters...      3   Business\n",
            "1  carlyle looks toward commercial aerospace reut...      3   Business\n",
            "2  oil and economy cloud stocks outlook reuters r...      3   Business\n",
            "3  iraq halts oil exports from main southern pipe...      3   Business\n",
            "4  oil prices soar to alltime record posing new m...      3   Business\n",
            "\n",
            "Class Distribution:\n",
            "class_name\n",
            "Business    30000\n",
            "Sci/Tech    30000\n",
            "Sports      30000\n",
            "World       30000\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "\n",
        "def preprocess_ag_news(file_path):\n",
        "    # 1. Load data - skipping the header row\n",
        "    df = pd.read_csv(file_path, header=0, names=['label', 'title', 'description'], engine='python')\n",
        "\n",
        "    # 2. Map numeric labels to names for better visualization\n",
        "    label_map = {1: 'World', 2: 'Sports', 3: 'Business', 4: 'Sci/Tech'}\n",
        "    df['class_name'] = df['label'].map(label_map)\n",
        "\n",
        "    # 3. Combine Title and Description\n",
        "    df['text'] = df['title'] + \" \" + df['description']\n",
        "\n",
        "    def clean_text(text):\n",
        "        # Lowercase\n",
        "        text = text.lower()\n",
        "        # Remove backslash escapes like \\n or \\b\n",
        "        text = re.sub(r'\\\\[nb]', ' ', text)\n",
        "        # Remove URLs\n",
        "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "        # Remove punctuation\n",
        "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "        # Remove extra whitespace\n",
        "        text = ' '.join(text.split())\n",
        "        return text\n",
        "\n",
        "    print(f\"Cleaning {file_path}...\")\n",
        "    df['text'] = df['text'].apply(clean_text)\n",
        "\n",
        "    # Keep only what is necessary for the models\n",
        "    return df[['text', 'label', 'class_name']]\n",
        "\n",
        "# EXECUTION\n",
        "train_cleaned = preprocess_ag_news('/content/drive/MyDrive/AG News Classification Dataset/train.csv')\n",
        "test_cleaned = preprocess_ag_news('/content/drive/MyDrive/AG News Classification Dataset/test.csv')\n",
        "\n",
        "# Save to shared CSVs\n",
        "train_cleaned.to_csv('ag_news_train_cleaned.csv', index=False)\n",
        "test_cleaned.to_csv('ag_news_test_cleaned.csv', index=False)\n",
        "\n",
        "print(\"\\nSample Output:\")\n",
        "print(train_cleaned.head())\n",
        "print(\"\\nClass Distribution:\")\n",
        "print(train_cleaned['class_name'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmac228Q5TM6",
        "outputId": "f26eb9b7-ae6b-4a72-d568-beaf7855578f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib-venn in /usr/local/lib/python3.12/dist-packages (1.1.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from matplotlib-venn) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from matplotlib-venn) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from matplotlib-venn) (1.16.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->matplotlib-venn) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->matplotlib-venn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->matplotlib-venn) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->matplotlib-venn) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->matplotlib-venn) (26.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->matplotlib-venn) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->matplotlib-venn) (3.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->matplotlib-venn) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->matplotlib-venn) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install matplotlib-venn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQrWPuUl5l-m",
        "outputId": "c5c54a52-df68-4a50-d5d9-99507ec0b722"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.1.0)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "def download_glove():\n",
        "    \"\"\"Download and extract GloVe embeddings\"\"\"\n",
        "    url = \"http://nlp.stanford.edu/data/glove.6B.zip\"\n",
        "    zip_file = \"glove.6B.zip\"\n",
        "\n",
        "    # Download if not exists\n",
        "    if not os.path.exists(zip_file):\n",
        "        print(\"Downloading GloVe embeddings (862 MB)...\")\n",
        "        urllib.request.urlretrieve(url, zip_file)\n",
        "        print(\"Download complete!\")\n",
        "\n",
        "    # Extract the 100d file\n",
        "    if not os.path.exists(\"glove.6B.100d.txt\"):\n",
        "        print(\"Extracting glove.6B.100d.txt...\")\n",
        "        with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "            zip_ref.extract(\"glove.6B.100d.txt\")\n",
        "        print(\"Extraction complete!\")\n",
        "\n",
        "    print(\"GloVe embeddings ready!\")\n",
        "\n",
        "# Run this before your main code\n",
        "download_glove()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8O2DJSBEHRH1",
        "outputId": "00b2f340-9d0e-4618-e21c-3ac75e63602e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading GloVe embeddings (862 MB)...\n",
            "Download complete!\n",
            "Extracting glove.6B.100d.txt...\n",
            "Extraction complete!\n",
            "GloVe embeddings ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fasttext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKYjFgxjU0OM",
        "outputId": "ff3058c0-cf27-4a91-ac52-279541d07fef"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.3.tar.gz (73 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/73.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-3.0.1-py3-none-any.whl.metadata (10.0 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from fasttext) (75.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from fasttext) (2.0.2)\n",
            "Using cached pybind11-3.0.1-py3-none-any.whl (293 kB)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.3-cp312-cp312-linux_x86_64.whl size=4498209 sha256=111a1761a139a262d37e8e5364b826d8abccab8236b7da495e4e74be7ba30eab\n",
            "  Stored in directory: /root/.cache/pip/wheels/20/27/95/a7baf1b435f1cbde017cabdf1e9688526d2b0e929255a359c6\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.3 pybind11-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout, Bidirectional\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from gensim.models import Word2Vec, FastText\n",
        "import re\n",
        "import pickle\n",
        "import time\n",
        "from typing import Tuple, Dict\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "# Create the directory\n",
        "output_path = Path('/mnt/user-data/outputs/')\n",
        "output_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Save your model\n",
        "# model.save(output_path / 'tfidf_lstm_model.keras') # This line was causing the NameError, as 'model' was not defined here.\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Constants\n",
        "MAX_WORDS = 20000\n",
        "MAX_SEQUENCE_LENGTH = 100\n",
        "EMBEDDING_DIM = 100\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 5  # Reduced to 5\n",
        "SAMPLE_SIZE = 0.2  # Use 20% of data for faster training\n",
        "\n",
        "class DataPreprocessor:\n",
        "    \"\"\"Handle data loading and preprocessing\"\"\"\n",
        "\n",
        "    def __init__(self, train_path: str, test_path: str):\n",
        "        self.train_path = train_path\n",
        "        self.test_path = test_path\n",
        "        self.tokenizer = None\n",
        "        self.label_encoder = None\n",
        "\n",
        "    def load_data(self) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "        \"\"\"Load train and test datasets with sampling\"\"\"\n",
        "        print(\"Loading datasets...\")\n",
        "        train_df = pd.read_csv(self.train_path)\n",
        "        test_df = pd.read_csv(self.test_path)\n",
        "\n",
        "        # Sample the data for faster training\n",
        "        print(f\"\\nOriginal train samples: {len(train_df)}\")\n",
        "        print(f\"Original test samples: {len(test_df)}\")\n",
        "\n",
        "        train_df = train_df.sample(frac=SAMPLE_SIZE, random_state=42).reset_index(drop=True)\n",
        "        test_df = test_df.sample(frac=SAMPLE_SIZE, random_state=42).reset_index(drop=True)\n",
        "\n",
        "        print(f\"\\nSampled train samples ({SAMPLE_SIZE*100}%): {len(train_df)}\")\n",
        "        print(f\"Sampled test samples ({SAMPLE_SIZE*100}%): {len(test_df)}\")\n",
        "\n",
        "        # AG News has columns: Class Index, Title, Description\n",
        "        # Combine Title and Description for better context\n",
        "        train_df['text'] = train_df['Title'] + ' ' + train_df['Description']\n",
        "        test_df['text'] = test_df['Title'] + ' ' + test_df['Description']\n",
        "\n",
        "        print(f\"Classes: {sorted(train_df['Class Index'].unique())}\")\n",
        "\n",
        "        return train_df, test_df\n",
        "\n",
        "    def clean_text(self, text: str) -> str:\n",
        "        \"\"\"Clean and preprocess text\"\"\"\n",
        "        # Convert to lowercase\n",
        "        text = text.lower()\n",
        "        # Remove special characters and digits\n",
        "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "        # Remove extra whitespace\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        return text\n",
        "\n",
        "    def prepare_data(self, train_df: pd.DataFrame, test_df: pd.DataFrame):\n",
        "        \"\"\"Prepare data for training\"\"\"\n",
        "        print(\"\\nCleaning text...\")\n",
        "        train_df['cleaned_text'] = train_df['text'].apply(self.clean_text)\n",
        "        test_df['cleaned_text'] = test_df['text'].apply(self.clean_text)\n",
        "\n",
        "        # Encode labels\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        y_train = self.label_encoder.fit_transform(train_df['Class Index'])\n",
        "        y_test = self.label_encoder.transform(test_df['Class Index'])\n",
        "\n",
        "        # Tokenize text\n",
        "        print(\"Tokenizing text...\")\n",
        "        self.tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token='<UNK>')\n",
        "        self.tokenizer.fit_on_texts(train_df['cleaned_text'])\n",
        "\n",
        "        # Convert to sequences\n",
        "        X_train_seq = self.tokenizer.texts_to_sequences(train_df['cleaned_text'])\n",
        "        X_test_seq = self.tokenizer.texts_to_sequences(test_df['cleaned_text'])\n",
        "\n",
        "        # Pad sequences\n",
        "        X_train_padded = pad_sequences(X_train_seq, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
        "        X_test_padded = pad_sequences(X_test_seq, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
        "\n",
        "        # One-hot encode labels\n",
        "        y_train_cat = to_categorical(y_train)\n",
        "        y_test_cat = to_categorical(y_test)\n",
        "\n",
        "        print(f\"Vocabulary size: {len(self.tokenizer.word_index)}\")\n",
        "        print(f\"X_train shape: {X_train_padded.shape}\")\n",
        "        print(f\"X_test shape: {X_test_padded.shape}\")\n",
        "\n",
        "        return X_train_padded, X_test_padded, y_train_cat, y_test_cat, train_df['cleaned_text'], test_df['cleaned_text']\n",
        "\n",
        "\n",
        "class TFIDFEmbedding:\n",
        "    \"\"\"Create TF-IDF weighted word embeddings\"\"\"\n",
        "\n",
        "    def __init__(self, tokenizer, vocab_size: int, embedding_dim: int):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.tfidf_vectorizer = None\n",
        "        self.embedding_matrix = None\n",
        "\n",
        "    def create_embedding_matrix(self, texts):\n",
        "        \"\"\"Create embedding matrix using TF-IDF weights\"\"\"\n",
        "        print(\"\\n=== Creating TF-IDF Embedding Matrix ===\")\n",
        "\n",
        "        # Calculate TF-IDF\n",
        "        self.tfidf_vectorizer = TfidfVectorizer(max_features=self.vocab_size)\n",
        "        self.tfidf_vectorizer.fit(texts)\n",
        "\n",
        "        # Initialize random embedding matrix\n",
        "        self.embedding_matrix = np.random.randn(self.vocab_size + 1, self.embedding_dim) * 0.01\n",
        "\n",
        "        # Weight embeddings by TF-IDF scores\n",
        "        word_index = self.tokenizer.word_index\n",
        "        tfidf_feature_names = self.tfidf_vectorizer.get_feature_names_out()\n",
        "        tfidf_vocab = {word: idx for idx, word in enumerate(tfidf_feature_names)}\n",
        "\n",
        "        for word, idx in word_index.items():\n",
        "            if idx < self.vocab_size and word in tfidf_vocab:\n",
        "                # Use TF-IDF score to scale the random embedding\n",
        "                tfidf_idx = tfidf_vocab[word]\n",
        "                tfidf_scores = self.tfidf_vectorizer.idf_[tfidf_idx]\n",
        "                self.embedding_matrix[idx] = np.random.randn(self.embedding_dim) * tfidf_scores * 0.01\n",
        "\n",
        "        print(f\"TF-IDF Embedding matrix shape: {self.embedding_matrix.shape}\")\n",
        "        return self.embedding_matrix\n",
        "\n",
        "\n",
        "class Word2VecEmbedding:\n",
        "    \"\"\"Create Word2Vec Skip-gram embeddings\"\"\"\n",
        "\n",
        "    def __init__(self, tokenizer, vocab_size: int, embedding_dim: int):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.w2v_model = None\n",
        "        self.embedding_matrix = None\n",
        "\n",
        "    def create_embedding_matrix(self, texts):\n",
        "        \"\"\"Create embedding matrix using Word2Vec Skip-gram\"\"\"\n",
        "        print(\"\\n=== Creating Word2Vec Skip-gram Embedding Matrix ===\")\n",
        "\n",
        "        # Tokenize texts into words\n",
        "        sentences = [text.split() for text in texts]\n",
        "\n",
        "        # Train Word2Vec model with Skip-gram (sg=1)\n",
        "        print(\"Training Word2Vec Skip-gram model...\")\n",
        "        self.w2v_model = Word2Vec(\n",
        "            sentences=sentences,\n",
        "            vector_size=self.embedding_dim,\n",
        "            window=5,\n",
        "            min_count=2,\n",
        "            workers=4,\n",
        "            sg=1,  # Skip-gram\n",
        "            epochs=10,\n",
        "            seed=42\n",
        "        )\n",
        "\n",
        "        # Create embedding matrix\n",
        "        self.embedding_matrix = np.zeros((self.vocab_size + 1, self.embedding_dim))\n",
        "        word_index = self.tokenizer.word_index\n",
        "\n",
        "        found = 0\n",
        "        for word, idx in word_index.items():\n",
        "            if idx < self.vocab_size:\n",
        "                try:\n",
        "                    self.embedding_matrix[idx] = self.w2v_model.wv[word]\n",
        "                    found += 1\n",
        "                except KeyError:\n",
        "                    # Word not in Word2Vec vocabulary, use random initialization\n",
        "                    self.embedding_matrix[idx] = np.random.randn(self.embedding_dim) * 0.01\n",
        "\n",
        "        print(f\"Word2Vec Embedding matrix shape: {self.embedding_matrix.shape}\")\n",
        "        print(f\"Found embeddings for {found}/{min(len(word_index), self.vocab_size)} words\")\n",
        "\n",
        "        return self.embedding_matrix\n",
        "\n",
        "\n",
        "class FastTextEmbedding:\n",
        "    \"\"\"Create FastText embeddings\"\"\"\n",
        "\n",
        "    def __init__(self, tokenizer, vocab_size: int, embedding_dim: int):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.fasttext_model = None\n",
        "        self.embedding_matrix = None\n",
        "\n",
        "    def create_embedding_matrix(self, texts):\n",
        "        \"\"\"Create embedding matrix using FastText\"\"\"\n",
        "        print(\"\\n=== Creating FastText Embedding Matrix ===\")\n",
        "\n",
        "        # Tokenize texts into words\n",
        "        sentences = [text.split() for text in texts]\n",
        "\n",
        "        # Train FastText model\n",
        "        print(\"Training FastText model...\")\n",
        "        self.fasttext_model = FastText(\n",
        "            sentences=sentences,\n",
        "            vector_size=self.embedding_dim,\n",
        "            window=5,\n",
        "            min_count=2,\n",
        "            workers=4,\n",
        "            sg=1,  # Skip-gram\n",
        "            epochs=10,\n",
        "            seed=42\n",
        "        )\n",
        "\n",
        "        # Create embedding matrix\n",
        "        self.embedding_matrix = np.zeros((self.vocab_size + 1, self.embedding_dim))\n",
        "        word_index = self.tokenizer.word_index\n",
        "\n",
        "        found = 0\n",
        "        for word, idx in word_index.items():\n",
        "            if idx < self.vocab_size:\n",
        "                try:\n",
        "                    # FastText can generate embeddings for OOV words using subword information\n",
        "                    self.embedding_matrix[idx] = self.fasttext_model.wv[word]\n",
        "                    found += 1\n",
        "                except KeyError:\n",
        "                    # Use random initialization as fallback\n",
        "                    self.embedding_matrix[idx] = np.random.randn(self.embedding_dim) * 0.01\n",
        "\n",
        "        print(f\"FastText Embedding matrix shape: {self.embedding_matrix.shape}\")\n",
        "        print(f\"Found embeddings for {found}/{min(len(word_index), self.vocab_size)} words\")\n",
        "\n",
        "        return self.embedding_matrix\n",
        "\n",
        "\n",
        "class GloVeEmbedding:\n",
        "    \"\"\"Load and use pre-trained GloVe embeddings\"\"\"\n",
        "\n",
        "    def __init__(self, tokenizer, vocab_size: int, embedding_dim: int, glove_path: str = None):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.glove_path = glove_path\n",
        "        self.embedding_matrix = None\n",
        "\n",
        "    def load_glove_embeddings(self):\n",
        "        \"\"\"Load GloVe embeddings from file\"\"\"\n",
        "        embeddings_index = {}\n",
        "\n",
        "        if self.glove_path is None:\n",
        "            print(\"\\nNote: GloVe file path not provided. Creating synthetic GloVe-style embeddings.\")\n",
        "            print(\"For real GloVe embeddings, download from: https://nlp.stanford.edu/projects/glove/\")\n",
        "            return None\n",
        "\n",
        "        print(f\"\\nLoading GloVe embeddings from {self.glove_path}...\")\n",
        "        with open(self.glove_path, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                values = line.split()\n",
        "                word = values[0]\n",
        "                vector = np.asarray(values[1:], dtype='float32')\n",
        "                embeddings_index[word] = vector\n",
        "\n",
        "        print(f\"Loaded {len(embeddings_index)} word vectors\")\n",
        "        return embeddings_index\n",
        "\n",
        "    def create_embedding_matrix(self, texts=None):\n",
        "        \"\"\"Create embedding matrix using GloVe\"\"\"\n",
        "        print(\"\\n=== Creating GloVe Embedding Matrix ===\")\n",
        "\n",
        "        embeddings_index = self.load_glove_embeddings()\n",
        "\n",
        "        # Initialize with zeros\n",
        "        self.embedding_matrix = np.zeros((self.vocab_size + 1, self.embedding_dim))\n",
        "        word_index = self.tokenizer.word_index\n",
        "\n",
        "        if embeddings_index is None:\n",
        "            # Create synthetic embeddings if GloVe file not available\n",
        "            print(\"Creating synthetic context-based embeddings...\")\n",
        "            for word, idx in word_index.items():\n",
        "                if idx < self.vocab_size:\n",
        "                    # Use hash-based deterministic initialization\n",
        "                    np.random.seed(hash(word) % (2**32))\n",
        "                    self.embedding_matrix[idx] = np.random.randn(self.embedding_dim) * 0.01\n",
        "        else:\n",
        "            # Use real GloVe embeddings\n",
        "            found = 0\n",
        "            for word, idx in word_index.items():\n",
        "                if idx < self.vocab_size:\n",
        "                    embedding_vector = embeddings_index.get(word)\n",
        "                    if embedding_vector is not None:\n",
        "                        self.embedding_matrix[idx] = embedding_vector\n",
        "                        found += 1\n",
        "                    else:\n",
        "                        # Random initialization for OOV words\n",
        "                        self.embedding_matrix[idx] = np.random.randn(self.embedding_dim) * 0.01\n",
        "\n",
        "            print(f\"Found embeddings for {found}/{min(len(word_index), self.vocab_size)} words\")\n",
        "\n",
        "        print(f\"GloVe Embedding matrix shape: {self.embedding_matrix.shape}\")\n",
        "        return self.embedding_matrix\n",
        "\n",
        "    def evaluate(self, X_test, y_test):\n",
        "        \"\"\"Evaluate the model\"\"\"\n",
        "        print(f\"\\n=== Evaluating {self.name} ===\")\n",
        "\n",
        "        loss, accuracy = self.model.evaluate(X_test, y_test, verbose=0)\n",
        "        print(f\"Test Loss: {loss:.4f}\")\n",
        "        print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "        return loss, accuracy\n",
        "\n",
        "\n",
        "def plot_training_history(histories: Dict, save_path: str = '/mnt/user-data/outputs/training_comparison.png'):\n",
        "    \"\"\"Plot training history comparison\"\"\"\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
        "\n",
        "    colors = {'TF-IDF': '#3498db', 'Skip-gram': '#e74c3c', 'FastText': '#9b59b6', 'GloVe': '#2ecc71'}\n",
        "\n",
        "    for name, history in histories.items():\n",
        "        color = colors.get(name, '#000000')\n",
        "        # Plot accuracy\n",
        "        axes[0].plot(history.history['accuracy'], label=f'{name} - Train',\n",
        "                    alpha=0.8, color=color, linewidth=2)\n",
        "        axes[0].plot(history.history['val_accuracy'], label=f'{name} - Val',\n",
        "                    linestyle='--', alpha=0.8, color=color, linewidth=2)\n",
        "\n",
        "    axes[0].set_title('Model Accuracy Comparison', fontsize=14, fontweight='bold')\n",
        "    axes[0].set_xlabel('Epoch')\n",
        "    axes[0].set_ylabel('Accuracy')\n",
        "    axes[0].legend(loc='lower right', fontsize=9)\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "    for name, history in histories.items():\n",
        "        color = colors.get(name, '#000000')\n",
        "        # Plot loss\n",
        "        axes[1].plot(history.history['loss'], label=f'{name} - Train',\n",
        "                    alpha=0.8, color=color, linewidth=2)\n",
        "        axes[1].plot(history.history['val_loss'], label=f'{name} - Val',\n",
        "                    linestyle='--', alpha=0.8, color=color, linewidth=2)\n",
        "\n",
        "    axes[1].set_title('Model Loss Comparison', fontsize=14, fontweight='bold')\n",
        "    axes[1].set_xlabel('Epoch')\n",
        "    axes[1].set_ylabel('Loss')\n",
        "    axes[1].legend(loc='upper right', fontsize=9)\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"\\nTraining history plot saved to {save_path}\")\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_results_comparison(results: Dict, save_path: str = '/mnt/user-data/outputs/results_comparison.png'):\n",
        "    \"\"\"Plot final results comparison\"\"\"\n",
        "    models = list(results.keys())\n",
        "    accuracies = [results[model]['accuracy'] for model in models]\n",
        "    losses = [results[model]['loss'] for model in models]\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    colors = ['#3498db', '#e74c3c', '#9b59b6', '#2ecc71']\n",
        "\n",
        "    # Accuracy comparison\n",
        "    bars1 = axes[0].bar(models, accuracies, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
        "    axes[0].set_title('Test Accuracy Comparison', fontsize=14, fontweight='bold')\n",
        "    axes[0].set_ylabel('Accuracy')\n",
        "    axes[0].set_ylim([0, 1])\n",
        "    axes[0].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for bar in bars1:\n",
        "        height = bar.get_height()\n",
        "        axes[0].text(bar.get_x() + bar.get_width()/2., height,\n",
        "                    f'{height:.4f}',\n",
        "                    ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
        "\n",
        "    # Loss comparison\n",
        "    bars2 = axes[1].bar(models, losses, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
        "    axes[1].set_title('Test Loss Comparison', fontsize=14, fontweight='bold')\n",
        "    axes[1].set_ylabel('Loss')\n",
        "    axes[1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for bar in bars2:\n",
        "        height = bar.get_height()\n",
        "        axes[1].text(bar.get_x() + bar.get_width()/2., height,\n",
        "                    f'{height:.4f}',\n",
        "                    ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"Results comparison plot saved to {save_path}\")\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main training pipeline\"\"\"\n",
        "    print(\"=\"*80)\n",
        "    print(\"LSTM TEXT CLASSIFICATION WITH MULTIPLE EMBEDDING APPROACHES\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"Epochs: {EPOCHS}\")\n",
        "    print(f\"Data sample size: {SAMPLE_SIZE*100}%\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Paths - UPDATE THESE WITH YOUR LOCAL PATHS\n",
        "    train_path = '/content/drive/MyDrive/AG News Classification Dataset/train.csv'\n",
        "    test_path = '/content/drive/MyDrive/AG News Classification Dataset/test.csv'\n",
        "    glove_path = None  # Set to 'glove.6B.100d.txt' if you have GloVe file\n",
        "\n",
        "    # 1. Load and preprocess data\n",
        "    preprocessor = DataPreprocessor(train_path, test_path)\n",
        "    train_df, test_df = preprocessor.load_data()\n",
        "    X_train, X_test, y_train, y_test, train_texts, test_texts = preprocessor.prepare_data(train_df, test_df)\n",
        "\n",
        "    # Split training data for validation\n",
        "    X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
        "        X_train, y_train, test_size=0.1, random_state=42\n",
        "    )\n",
        "\n",
        "    vocab_size = min(len(preprocessor.tokenizer.word_index), MAX_WORDS)\n",
        "    num_classes = y_train.shape[1]\n",
        "\n",
        "    # Store results\n",
        "    histories = {}\n",
        "    results = {}\n",
        "\n",
        "    # 2. TF-IDF Embedding Approach\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"APPROACH 1: TF-IDF WEIGHTED EMBEDDINGS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    tfidf_emb = TFIDFEmbedding(preprocessor.tokenizer, vocab_size, EMBEDDING_DIM)\n",
        "    tfidf_matrix = tfidf_emb.create_embedding_matrix(train_texts)\n",
        "\n",
        "    # Define the LSTMClassifier class first\n",
        "    class LSTMClassifier:\n",
        "        def __init__(self, vocab_size, embedding_dim, num_classes, name=\"LSTM_Model\"):\n",
        "            self.vocab_size = vocab_size\n",
        "            self.embedding_dim = embedding_dim\n",
        "            self.num_classes = num_classes\n",
        "            self.name = name\n",
        "            self.model = None\n",
        "\n",
        "        def build_model(self, embedding_matrix=None, trainable_embeddings=True):\n",
        "            model = Sequential()\n",
        "            if embedding_matrix is not None:\n",
        "                model.add(Embedding(self.vocab_size + 1, self.embedding_dim, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=trainable_embeddings))\n",
        "            else:\n",
        "                model.add(Embedding(self.vocab_size + 1, self.embedding_dim, input_length=MAX_SEQUENCE_LENGTH))\n",
        "            model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
        "            model.add(Dropout(0.3))\n",
        "            model.add(Bidirectional(LSTM(32)))\n",
        "            model.add(Dropout(0.3))\n",
        "            model.add(Dense(self.num_classes, activation='softmax'))\n",
        "\n",
        "            model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "            self.model = model\n",
        "            print(f\"\\n{self.name} Model Summary:\")\n",
        "            self.model.summary()\n",
        "\n",
        "        def train(self, X_train, y_train, X_val, y_val):\n",
        "            print(f\"\\n=== Training {self.name} ===\")\n",
        "            early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "            reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.0001)\n",
        "            history = self.model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(X_val, y_val), callbacks=[early_stopping, reduce_lr], verbose=1)\n",
        "            return history\n",
        "\n",
        "        def evaluate(self, X_test, y_test):\n",
        "            print(f\"\\n=== Evaluating {self.name} ===\")\n",
        "            loss, accuracy = self.model.evaluate(X_test, y_test, verbose=0)\n",
        "            print(f\"Test Loss: {loss:.4f}\")\n",
        "            print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "            return loss, accuracy\n",
        "\n",
        "    tfidf_model = LSTMClassifier(vocab_size, EMBEDDING_DIM, num_classes, \"TF-IDF LSTM\")\n",
        "    tfidf_model.build_model(embedding_matrix=tfidf_matrix, trainable_embeddings=True)\n",
        "    tfidf_history = tfidf_model.train(X_train_split, y_train_split, X_val, y_val)\n",
        "    tfidf_loss, tfidf_acc = tfidf_model.evaluate(X_test, y_test)\n",
        "\n",
        "    histories['TF-IDF'] = tfidf_history\n",
        "    results['TF-IDF'] = {'loss': tfidf_loss, 'accuracy': tfidf_acc}\n",
        "\n",
        "    # Save model\n",
        "    tfidf_model.model.save('/mnt/user-data/outputs/tfidf_lstm_model.keras')\n",
        "\n",
        "    # 3. Word2Vec Skip-gram Approach\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"APPROACH 2: WORD2VEC SKIP-GRAM EMBEDDINGS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    w2v_emb = Word2VecEmbedding(preprocessor.tokenizer, vocab_size, EMBEDDING_DIM)\n",
        "    w2v_matrix = w2v_emb.create_embedding_matrix(train_texts)\n",
        "\n",
        "    w2v_model = LSTMClassifier(vocab_size, EMBEDDING_DIM, num_classes, \"Skip-gram LSTM\")\n",
        "    w2v_model.build_model(embedding_matrix=w2v_matrix, trainable_embeddings=True)\n",
        "    w2v_history = w2v_model.train(X_train_split, y_train_split, X_val, y_val)\n",
        "    w2v_loss, w2v_acc = w2v_model.evaluate(X_test, y_test)\n",
        "\n",
        "    histories['Skip-gram'] = w2v_history\n",
        "    results['Skip-gram'] = {'loss': w2v_loss, 'accuracy': w2v_acc}\n",
        "\n",
        "    # Save model\n",
        "    w2v_model.model.save('/mnt/user-data/outputs/skipgram_lstm_model.keras')\n",
        "\n",
        "    # 4. FastText Approach\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"APPROACH 3: FASTTEXT EMBEDDINGS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    fasttext_emb = FastTextEmbedding(preprocessor.tokenizer, vocab_size, EMBEDDING_DIM)\n",
        "    fasttext_matrix = fasttext_emb.create_embedding_matrix(train_texts)\n",
        "\n",
        "    fasttext_model = LSTMClassifier(vocab_size, EMBEDDING_DIM, num_classes, \"FastText LSTM\")\n",
        "    fasttext_model.build_model(embedding_matrix=fasttext_matrix, trainable_embeddings=True)\n",
        "    fasttext_history = fasttext_model.train(X_train_split, y_train_split, X_val, y_val)\n",
        "    fasttext_loss, fasttext_acc = fasttext_model.evaluate(X_test, y_test)\n",
        "\n",
        "    histories['FastText'] = fasttext_history\n",
        "    results['FastText'] = {'loss': fasttext_loss, 'accuracy': fasttext_acc}\n",
        "\n",
        "    # Save model\n",
        "    fasttext_model.model.save('/mnt/user-data/outputs/fasttext_lstm_model.keras')\n",
        "\n",
        "    # 5. GloVe Embedding Approach\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"APPROACH 4: GLOVE EMBEDDINGS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    glove_emb = GloVeEmbedding(preprocessor.tokenizer, vocab_size, EMBEDDING_DIM, glove_path)\n",
        "    glove_matrix = glove_emb.create_embedding_matrix(train_texts)\n",
        "\n",
        "    glove_model = LSTMClassifier(vocab_size, EMBEDDING_DIM, num_classes, \"GloVe LSTM\")\n",
        "    glove_model.build_model(embedding_matrix=glove_matrix, trainable_embeddings=False)\n",
        "    glove_history = glove_model.train(X_train_split, y_train_split, X_val, y_val)\n",
        "    glove_loss, glove_acc = glove_model.evaluate(X_test, y_test)\n",
        "\n",
        "    histories['GloVe'] = glove_history\n",
        "    results['GloVe'] = {'loss': glove_loss, 'accuracy': glove_acc}\n",
        "\n",
        "    # Save model\n",
        "    glove_model.model.save('/mnt/user-data/outputs/glove_lstm_model.keras')\n",
        "\n",
        "    # 6. Generate comparison plots\n",
        "    plot_training_history(histories)\n",
        "    plot_results_comparison(results)\n",
        "\n",
        "    # 7. Save results summary\n",
        "    results_summary = pd.DataFrame(results).T\n",
        "    results_summary = results_summary.sort_values('accuracy', ascending=False)\n",
        "    results_summary.to_csv('/mnt/user-data/outputs/results_summary.csv')\n",
        "\n",
        "    # 8. Print final summary\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"FINAL RESULTS SUMMARY\")\n",
        "    print(\"=\"*80)\n",
        "    print(results_summary)\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"Best Model: {results_summary.index[0]}\")\n",
        "    print(f\"Best Accuracy: {results_summary['accuracy'].iloc[0]:.4f}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Save preprocessing objects\n",
        "    with open('/mnt/user-data/outputs/tokenizer.pkl', 'wb') as f:\n",
        "        pickle.dump(preprocessor.tokenizer, f)\n",
        "\n",
        "    with open('/mnt/user-data/outputs/label_encoder.pkl', 'wb') as f:\n",
        "        pickle.dump(preprocessor.label_encoder, f)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"TRAINING COMPLETE!\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"\\nSaved files:\")\n",
        "    print(\" - tfidf_lstm_model.keras\")\n",
        "    print(\" - skipgram_lstm_model.keras\")\n",
        "    print(\" - fasttext_lstm_model.keras\")\n",
        "    print(\" - glove_lstm_model.keras\")\n",
        "    print(\" - training_comparison.png\")\n",
        "    print(\" - results_comparison.png\")\n",
        "    print(\" - results_summary.csv\")\n",
        "    print(\" - tokenizer.pkl\")\n",
        "    print(\" - label_encoder.pkl\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QSggYhUvhKC8",
        "outputId": "76d6618f-9f90-4580-e9da-737384ddbb21"
      },
      "execution_count": 8,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "LSTM TEXT CLASSIFICATION WITH MULTIPLE EMBEDDING APPROACHES\n",
            "================================================================================\n",
            "Epochs: 5\n",
            "Data sample size: 20.0%\n",
            "================================================================================\n",
            "Loading datasets...\n",
            "\n",
            "Original train samples: 120000\n",
            "Original test samples: 7600\n",
            "\n",
            "Sampled train samples (20.0%): 24000\n",
            "Sampled test samples (20.0%): 1520\n",
            "Classes: [np.int64(1), np.int64(2), np.int64(3), np.int64(4)]\n",
            "\n",
            "Cleaning text...\n",
            "Tokenizing text...\n",
            "Vocabulary size: 42852\n",
            "X_train shape: (24000, 100)\n",
            "X_test shape: (1520, 100)\n",
            "\n",
            "================================================================================\n",
            "APPROACH 1: TF-IDF WEIGHTED EMBEDDINGS\n",
            "================================================================================\n",
            "\n",
            "=== Creating TF-IDF Embedding Matrix ===\n",
            "TF-IDF Embedding matrix shape: (20001, 100)\n",
            "\n",
            "TF-IDF LSTM Model Summary:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,000,100</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │     \u001b[38;5;34m2,000,100\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m) │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,000,100</span> (7.63 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,000,100\u001b[0m (7.63 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,000,100</span> (7.63 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,000,100\u001b[0m (7.63 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Training TF-IDF LSTM ===\n",
            "Epoch 1/5\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 767ms/step - accuracy: 0.5816 - loss: 0.9945 - val_accuracy: 0.8779 - val_loss: 0.3912 - learning_rate: 0.0010\n",
            "Epoch 2/5\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 680ms/step - accuracy: 0.9106 - loss: 0.2962 - val_accuracy: 0.8821 - val_loss: 0.3675 - learning_rate: 0.0010\n",
            "Epoch 3/5\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 684ms/step - accuracy: 0.9517 - loss: 0.1769 - val_accuracy: 0.8796 - val_loss: 0.4048 - learning_rate: 0.0010\n",
            "Epoch 4/5\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 668ms/step - accuracy: 0.9691 - loss: 0.1193 - val_accuracy: 0.8621 - val_loss: 0.5000 - learning_rate: 0.0010\n",
            "Epoch 5/5\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 721ms/step - accuracy: 0.9787 - loss: 0.0853 - val_accuracy: 0.8792 - val_loss: 0.4687 - learning_rate: 2.0000e-04\n",
            "\n",
            "=== Evaluating TF-IDF LSTM ===\n",
            "Test Loss: 0.3748\n",
            "Test Accuracy: 0.8816\n",
            "\n",
            "================================================================================\n",
            "APPROACH 2: WORD2VEC SKIP-GRAM EMBEDDINGS\n",
            "================================================================================\n",
            "\n",
            "=== Creating Word2Vec Skip-gram Embedding Matrix ===\n",
            "Training Word2Vec Skip-gram model...\n",
            "Word2Vec Embedding matrix shape: (20001, 100)\n",
            "Found embeddings for 19998/20000 words\n",
            "\n",
            "Skip-gram LSTM Model Summary:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,000,100</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │     \u001b[38;5;34m2,000,100\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional_2 (\u001b[38;5;33mBidirectional\u001b[0m) │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional_3 (\u001b[38;5;33mBidirectional\u001b[0m) │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,000,100</span> (7.63 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,000,100\u001b[0m (7.63 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,000,100</span> (7.63 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,000,100\u001b[0m (7.63 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Training Skip-gram LSTM ===\n",
            "Epoch 1/5\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 677ms/step - accuracy: 0.7553 - loss: 0.7155 - val_accuracy: 0.8908 - val_loss: 0.3387 - learning_rate: 0.0010\n",
            "Epoch 2/5\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 684ms/step - accuracy: 0.9047 - loss: 0.3003 - val_accuracy: 0.8946 - val_loss: 0.3247 - learning_rate: 0.0010\n",
            "Epoch 3/5\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 777ms/step - accuracy: 0.9288 - loss: 0.2347 - val_accuracy: 0.8950 - val_loss: 0.3252 - learning_rate: 0.0010\n",
            "Epoch 4/5\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 684ms/step - accuracy: 0.9458 - loss: 0.1833 - val_accuracy: 0.8971 - val_loss: 0.3267 - learning_rate: 0.0010\n",
            "Epoch 5/5\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 723ms/step - accuracy: 0.9609 - loss: 0.1386 - val_accuracy: 0.9021 - val_loss: 0.3363 - learning_rate: 2.0000e-04\n",
            "\n",
            "=== Evaluating Skip-gram LSTM ===\n",
            "Test Loss: 0.3035\n",
            "Test Accuracy: 0.8947\n",
            "\n",
            "================================================================================\n",
            "APPROACH 3: FASTTEXT EMBEDDINGS\n",
            "================================================================================\n",
            "\n",
            "=== Creating FastText Embedding Matrix ===\n",
            "Training FastText model...\n",
            "FastText Embedding matrix shape: (20001, 100)\n",
            "Found embeddings for 19999/20000 words\n",
            "\n",
            "FastText LSTM Model Summary:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,000,100</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │     \u001b[38;5;34m2,000,100\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional_4 (\u001b[38;5;33mBidirectional\u001b[0m) │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional_5 (\u001b[38;5;33mBidirectional\u001b[0m) │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,000,100</span> (7.63 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,000,100\u001b[0m (7.63 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,000,100</span> (7.63 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,000,100\u001b[0m (7.63 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Training FastText LSTM ===\n",
            "Epoch 1/5\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 700ms/step - accuracy: 0.7721 - loss: 0.6964 - val_accuracy: 0.8838 - val_loss: 0.3485 - learning_rate: 0.0010\n",
            "Epoch 2/5\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 683ms/step - accuracy: 0.9032 - loss: 0.3118 - val_accuracy: 0.8929 - val_loss: 0.3227 - learning_rate: 0.0010\n",
            "Epoch 3/5\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 679ms/step - accuracy: 0.9281 - loss: 0.2365 - val_accuracy: 0.8958 - val_loss: 0.3246 - learning_rate: 0.0010\n",
            "Epoch 4/5\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 725ms/step - accuracy: 0.9447 - loss: 0.1881 - val_accuracy: 0.8908 - val_loss: 0.3627 - learning_rate: 0.0010\n",
            "Epoch 5/5\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 693ms/step - accuracy: 0.9587 - loss: 0.1422 - val_accuracy: 0.9054 - val_loss: 0.3369 - learning_rate: 2.0000e-04\n",
            "\n",
            "=== Evaluating FastText LSTM ===\n",
            "Test Loss: 0.3175\n",
            "Test Accuracy: 0.8914\n",
            "\n",
            "================================================================================\n",
            "APPROACH 4: GLOVE EMBEDDINGS\n",
            "================================================================================\n",
            "\n",
            "=== Creating GloVe Embedding Matrix ===\n",
            "\n",
            "Note: GloVe file path not provided. Creating synthetic GloVe-style embeddings.\n",
            "For real GloVe embeddings, download from: https://nlp.stanford.edu/projects/glove/\n",
            "Creating synthetic context-based embeddings...\n",
            "GloVe Embedding matrix shape: (20001, 100)\n",
            "\n",
            "GloVe LSTM Model Summary:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_3 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │     \u001b[38;5;34m2,000,100\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional_6 (\u001b[38;5;33mBidirectional\u001b[0m) │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional_7 (\u001b[38;5;33mBidirectional\u001b[0m) │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,000,100</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,000,100\u001b[0m (7.63 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,000,100</span> (7.63 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,000,100\u001b[0m (7.63 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,000,100</span> (7.63 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Training GloVe LSTM ===\n",
            "Epoch 1/5\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 596ms/step - accuracy: 0.2851 - loss: 1.3674 - val_accuracy: 0.4038 - val_loss: 1.2750 - learning_rate: 0.0010\n",
            "Epoch 2/5\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 584ms/step - accuracy: 0.4130 - loss: 1.2504 - val_accuracy: 0.5117 - val_loss: 1.1531 - learning_rate: 0.0010\n",
            "Epoch 3/5\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 586ms/step - accuracy: 0.5131 - loss: 1.1356 - val_accuracy: 0.5258 - val_loss: 1.1262 - learning_rate: 0.0010\n",
            "Epoch 4/5\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 587ms/step - accuracy: 0.5317 - loss: 1.1037 - val_accuracy: 0.5333 - val_loss: 1.1081 - learning_rate: 0.0010\n",
            "Epoch 5/5\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 583ms/step - accuracy: 0.5527 - loss: 1.0726 - val_accuracy: 0.5433 - val_loss: 1.0859 - learning_rate: 0.0010\n",
            "\n",
            "=== Evaluating GloVe LSTM ===\n",
            "Test Loss: 1.0784\n",
            "Test Accuracy: 0.5467\n",
            "\n",
            "Training history plot saved to /mnt/user-data/outputs/training_comparison.png\n",
            "Results comparison plot saved to /mnt/user-data/outputs/results_comparison.png\n",
            "\n",
            "================================================================================\n",
            "FINAL RESULTS SUMMARY\n",
            "================================================================================\n",
            "               loss  accuracy\n",
            "Skip-gram  0.303470  0.894737\n",
            "FastText   0.317470  0.891447\n",
            "TF-IDF     0.374796  0.881579\n",
            "GloVe      1.078433  0.546711\n",
            "\n",
            "================================================================================\n",
            "Best Model: Skip-gram\n",
            "Best Accuracy: 0.8947\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "TRAINING COMPLETE!\n",
            "================================================================================\n",
            "\n",
            "Saved files:\n",
            " - tfidf_lstm_model.keras\n",
            " - skipgram_lstm_model.keras\n",
            " - fasttext_lstm_model.keras\n",
            " - glove_lstm_model.keras\n",
            " - training_comparison.png\n",
            " - results_comparison.png\n",
            " - results_summary.csv\n",
            " - tokenizer.pkl\n",
            " - label_encoder.pkl\n",
            "================================================================================\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}